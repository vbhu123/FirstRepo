{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.65.60:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app-f1c2436e0bdfdda937ff66796bc18e1c-79646775d-5pzwg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=app-f1c2436e0bdfdda937ff66796bc18e1c-79646775d-5pzwg>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if we are connected to the Spark cluster\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Importing useful libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Logical Operations`\n",
    "\n",
    "  * Filter products based on value\n",
    "  \n",
    "  \n",
    "  * Create a flag based on values\n",
    "  \n",
    "  \n",
    "  * Modifying/ Creating columns using a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling datasets from GCP\n",
    "customers = sqlContext.table(\"an_training_easl.training1_cards\")\n",
    "products = sqlContext.table(\"an_training_easl.training1_products\")\n",
    "transactions = sqlContext.table(\"an_training_easl.training1_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-------------+\n",
      "|prod_id|category_id|department|category_desc|\n",
      "+-------+-----------+----------+-------------+\n",
      "|     P1|         C1|        D1|         Milk|\n",
      "|     P3|         C3|        D1|       yogurt|\n",
      "|     P8|         C1|        D1|         Milk|\n",
      "|    P10|         C3|        D1|       yogurt|\n",
      "|    P15|         C1|        D1|         Milk|\n",
      "|    P17|         C3|        D1|       yogurt|\n",
      "|    P22|         C1|        D1|         Milk|\n",
      "|    P24|         C3|        D1|       yogurt|\n",
      "|    P29|         C1|        D1|         Milk|\n",
      "|    P31|         C3|        D1|       yogurt|\n",
      "+-------+-----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subsetting dataset based on values\n",
    "prod_sub = products.where((F.col(\"category_id\") == \"C1\") | (F.col(\"category_desc\") == \"yogurt\"))\n",
    "prod_sub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-------------+----+\n",
      "|prod_id|category_id|department|category_desc|Flag|\n",
      "+-------+-----------+----------+-------------+----+\n",
      "|     P1|         C1|        D1|         Milk|   1|\n",
      "|     P2|         C2|        D2|      Shampoo|   0|\n",
      "|     P3|         C3|        D1|       yogurt|   1|\n",
      "|     P4|         C4|        D2|         soap|   0|\n",
      "|     P5|         C5|        D2|floor cleaner|   0|\n",
      "+-------+-----------+----------+-------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a column called Flag based on certain values/ like case when in SQL\n",
    "prod_sub = products.withColumn(\"Flag\", F.when((F.col(\"category_id\") == \"C1\")|\\\n",
    "                                              (F.col(\"category_desc\") == \"yogurt\"), F.lit(1)) \\\n",
    "                                       .otherwise(F.lit(0))\n",
    "                              )\n",
    "prod_sub.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------+-----------------+\n",
      "|col_prod_id|col_category_id|col_department|col_category_desc|\n",
      "+-----------+---------------+--------------+-----------------+\n",
      "|         P1|             C1|            D1|             Milk|\n",
      "|         P2|             C2|            D2|          Shampoo|\n",
      "|         P3|             C3|            D1|           yogurt|\n",
      "|         P4|             C4|            D2|             soap|\n",
      "|         P5|             C5|            D2|    floor cleaner|\n",
      "+-----------+---------------+--------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can be skipped as this uses for loops and list\n",
    "# Pre-appending 'col_' before each column\n",
    "prod_sub.select([F.col(column).alias('col_'+column) for column in products.columns if column in prod_sub.columns]).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Group by and aggregations on Dataframes`\n",
    "\n",
    "  * Find distribution\n",
    "  \n",
    "  \n",
    "      * number of values in each column\n",
    "      * maximum and minimun of each column\n",
    "      * Average values\n",
    "      \n",
    "      \n",
    "  * Rename columns using withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+--------------+\n",
      "|shabits|max(income)|avg(age)|count(card_id)|\n",
      "+-------+-----------+--------+--------------+\n",
      "|     VL|   999432.0|    49.6|             5|\n",
      "|     Po|    55674.0|    31.0|             1|\n",
      "|     Vl|    50000.0|    25.0|             1|\n",
      "|     PO|   558700.0|    38.0|             6|\n",
      "|     LP|    20000.0|    24.0|             1|\n",
      "|     PR|  1000000.0|    40.4|             5|\n",
      "+-------+-----------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarizing dataframe at a shabit level and finding count, max and average values\n",
    "shabits_dist1 = customers.groupBy('shabits') \\\n",
    "                         .agg({'card_id': 'count',\n",
    "                               'income' : 'max',\n",
    "                               'age'    : 'mean'})\n",
    "shabits_dist1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+----------+-----------+\n",
      "|shabits|customers|max_income|min_income|average_age|\n",
      "+-------+---------+----------+----------+-----------+\n",
      "|     VL|        5|  999432.0|   45342.0|       49.6|\n",
      "|     Po|        1|   55674.0|   55674.0|       31.0|\n",
      "|     Vl|        1|   50000.0|   50000.0|       25.0|\n",
      "|     PO|        6|  558700.0|   12000.0|       38.0|\n",
      "|     LP|        1|   20000.0|   20000.0|       24.0|\n",
      "|     PR|        5| 1000000.0|   21098.0|       40.4|\n",
      "+-------+---------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In case of multiple aggregation on single column, dict is not useful\n",
    "shabits_dist2 = customers.groupBy('shabits')\\\n",
    "                         .agg(F.count('card_id').alias('customers'), \\\n",
    "                              F.max('income').alias('max_income'), \\\n",
    "                              F.min('income').alias('min_income'), \\\n",
    "                              F.mean('age').alias('average_age'))\n",
    "shabits_dist2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------------+\n",
      "|shabits|max_income|avg_age|# of Customers|\n",
      "+-------+----------+-------+--------------+\n",
      "|     VL|  999432.0|   49.6|             5|\n",
      "|     Po|   55674.0|   31.0|             1|\n",
      "|     Vl|   50000.0|   25.0|             1|\n",
      "|     PO|  558700.0|   38.0|             6|\n",
      "|     LP|   20000.0|   24.0|             1|\n",
      "|     PR| 1000000.0|   40.4|             5|\n",
      "+-------+----------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename aggregated columns using withColumnRenamed or select with alias\n",
    "shabits_dist_renamed = shabits_dist1.withColumnRenamed('max(income)', 'max_income') \\\n",
    "                                    .withColumnRenamed('avg(age)', 'avg_age') \\\n",
    "                                    .withColumnRenamed('count(card_id)', '# of Customers')\n",
    "shabits_dist_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Joins On DataFrames`\n",
    "  \n",
    "  * Join 2 different dataframes together\n",
    "  \n",
    "  \n",
    "  * Find out summarized value on joined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- date_id: string (nullable = true)\n",
      " |-- card_id: long (nullable = true)\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- week_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema of transaction dataframe\n",
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- category_desc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema of products dataframe\n",
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|transaction_id|   date_id|card_id|prod_id|qty|amount|week_id|prod_id|category_id|department|category_desc|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|      10134103|01-05-2017|    101|    P20|  1|  45.0| 201718|    P20|         C6|        D3|     biscuits|\n",
      "|      10162849|01-05-2017|    103|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10187085|01-05-2017|    105|    P31|  1|  28.0| 201718|    P31|         C3|        D1|       yogurt|\n",
      "|      10413355|02-05-2017|    107|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      10543839|02-05-2017|    111|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10600256|02-05-2017|    113|    P25|  1|  40.0| 201718|    P25|         C4|        D2|         soap|\n",
      "|      10664653|02-05-2017|    115|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      11026889|02-05-2017|    117|    P23|  1| 250.0| 201718|    P23|         C2|        D2|      Shampoo|\n",
      "|      11026939|02-05-2017|    120|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      11221390|02-05-2017|    122|     P2|  1| 250.0| 201718|     P2|         C2|        D2|      Shampoo|\n",
      "|      11238203|03-05-2017|    123|    P24|  1|  35.0| 201718|    P24|         C3|        D1|       yogurt|\n",
      "|      11450230|03-05-2017|    127|    P12|  1| 110.0| 201718|    P12|         C5|        D2|floor cleaner|\n",
      "|      11531607|03-05-2017|    133|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      11620341|03-05-2017|    137|     P9|  1| 190.0| 201718|     P9|         C2|        D2|      Shampoo|\n",
      "|      11768479|03-05-2017|    191|    P34|  1|  70.0| 201718|    P34|         C6|        D3|     biscuits|\n",
      "|      12381525|04-05-2017|    101|    P29|  1|  19.0| 201718|    P29|         C1|        D1|         Milk|\n",
      "|      12389296|04-05-2017|    103|    P15|  1|  20.0| 201718|    P15|         C1|        D1|         Milk|\n",
      "|      13088125|04-05-2017|    105|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      13229377|04-05-2017|    107|     P7|  1|  35.0| 201718|     P7|         C7|        D3|         soda|\n",
      "|      13372471|04-05-2017|    111|    P16|  1| 111.0| 201718|    P16|         C2|        D2|      Shampoo|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining transactions and products dataframe together using prod_id as the join key\n",
    "prod_trans = transactions.join(products, transactions.prod_id == products.prod_id, 'inner')\n",
    "prod_trans.show()\n",
    "\n",
    "#For 'left', 'right', 'outer' -  replace 'inner' with the needed join type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------+-------+\n",
      "|category_desc|category_id|tot_amount|tot_qty|\n",
      "+-------------+-----------+----------+-------+\n",
      "|         soda|         C7|    2470.0|     31|\n",
      "|         soap|         C4|     760.0|     22|\n",
      "|       yogurt|         C3|     952.0|     25|\n",
      "|      Shampoo|         C2|    6121.0|     33|\n",
      "|floor cleaner|         C5|    3750.0|     29|\n",
      "|     biscuits|         C6|    1700.0|     38|\n",
      "|         Milk|         C1|     765.0|     38|\n",
      "+-------------+-----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining datasets and summarizing/ aggregating values\n",
    "prod_trans2 = transactions.join(products, transactions.prod_id == products.prod_id, 'inner') \\\n",
    "                          .groupBy(products.category_desc, products.category_id) \\\n",
    "                          .agg(F.sum('amount').alias(\"tot_amount\"), F.sum('qty').alias(\"tot_qty\"))\n",
    "prod_trans2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Advanced Functionalities`\n",
    "  \n",
    "  * Summarizing values based on several filters and Creating pivot\n",
    "  \n",
    "  \n",
    "  * Creating pivot on multiple indexes\n",
    "  \n",
    "  \n",
    "  * Creating pivot with multiple aggregations\n",
    "  \n",
    "  \n",
    "  * Pivot with filtered values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------+---+--------------+----------+-------------+\n",
      "|prod_id|week_id|card_id|amount|qty|transaction_id|department|category_desc|\n",
      "+-------+-------+-------+------+---+--------------+----------+-------------+\n",
      "|    P20| 201718|    101|  45.0|  1|      10134103|        D3|     biscuits|\n",
      "|    P35| 201718|    103| 120.0|  1|      10162849|        D3|         soda|\n",
      "|    P31| 201718|    105|  28.0|  1|      10187085|        D1|       yogurt|\n",
      "|    P13| 201718|    107|  70.0|  1|      10413355|        D3|     biscuits|\n",
      "|    P35| 201718|    111| 120.0|  1|      10543839|        D3|         soda|\n",
      "|    P25| 201718|    113|  40.0|  1|      10600256|        D2|         soap|\n",
      "|    P33| 201718|    115| 110.0|  1|      10664653|        D2|floor cleaner|\n",
      "|    P23| 201718|    117| 250.0|  1|      11026889|        D2|      Shampoo|\n",
      "|    P13| 201718|    120|  70.0|  1|      11026939|        D3|     biscuits|\n",
      "|     P2| 201718|    122| 250.0|  1|      11221390|        D2|      Shampoo|\n",
      "+-------+-------+-------+------+---+--------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a transaction table with selected columns and data filtered on weeks and card_id\n",
    "trans=sqlContext.table('an_training_easl.training1_transactions') \\\n",
    "                    .select('week_id','prod_id','card_id', 'amount', 'qty','transaction_id') \\\n",
    "                    .where((F.col('week_id').between(201718,201719)) & \n",
    "                           (F.col('card_id').isNotNull()))\n",
    "\n",
    "# Picking up non-duplicate values from products table\n",
    "prods=sqlContext.table('an_training_easl.training1_products') \\\n",
    "                    .select('prod_id','department','category_desc') \\\n",
    "                    .dropDuplicates()\n",
    "\n",
    "# Joining the datasets together\n",
    "df = trans.join(broadcast(prods),\"prod_id\",\"inner\") \n",
    "df.show(10)\n",
    "### We'll see what a broadcast join is later, for the time being consider it just as a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+\n",
      "|category_desc|201718|201719|\n",
      "+-------------+------+------+\n",
      "|         Milk| 300.0| 465.0|\n",
      "|       yogurt| 417.0| 535.0|\n",
      "|         soap| 280.0| 480.0|\n",
      "|floor cleaner|2710.0|1040.0|\n",
      "|         soda|1630.0| 840.0|\n",
      "|      Shampoo|3148.0|2973.0|\n",
      "|     biscuits| 974.0| 726.0|\n",
      "+-------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivoting the above dataset on category description and adding up the amounts\n",
    "pivot_df = df.groupby('category_desc').pivot('week_id').sum('amount')\n",
    "pivot_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+\n",
      "|category_desc|department|201718|201719|\n",
      "+-------------+----------+------+------+\n",
      "|         Milk|        D1| 300.0| 465.0|\n",
      "|      Shampoo|        D2|3148.0|2973.0|\n",
      "|     biscuits|        D3| 974.0| 726.0|\n",
      "|floor cleaner|        D2|2710.0|1040.0|\n",
      "|         soap|        D2| 280.0| 480.0|\n",
      "|         soda|        D3|1630.0| 840.0|\n",
      "|       yogurt|        D1| 417.0| 535.0|\n",
      "+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivoting with multiple indexes\n",
    "pivot_df = df.groupby('category_desc','department').pivot('week_id').sum('amount').sort('category_desc')\n",
    "pivot_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+----------+------------+\n",
      "|category_desc|201718_sum|201718_count|201719_sum|201719_count|\n",
      "+-------------+----------+------------+----------+------------+\n",
      "|         Milk|     300.0|           5|     465.0|           5|\n",
      "|       yogurt|     417.0|           5|     535.0|           5|\n",
      "|         soap|     280.0|           4|     480.0|           4|\n",
      "|floor cleaner|    2710.0|           5|    1040.0|           5|\n",
      "|         soda|    1630.0|           5|     840.0|           5|\n",
      "|      Shampoo|    3148.0|           5|    2973.0|           5|\n",
      "|     biscuits|     974.0|           5|     726.0|           5|\n",
      "+-------------+----------+------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple aggregations using pivot on the same dataset\n",
    "pivot_df = df.groupby('category_desc') \\\n",
    "             .pivot('week_id') \\\n",
    "             .agg(F.sum('amount').alias('sum'), \\\n",
    "                  F.countDistinct('prod_id').alias('count'))\n",
    "\n",
    "pivot_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+\n",
      "|category_desc|201718_sum|201718_count|\n",
      "+-------------+----------+------------+\n",
      "|         Milk|     300.0|           5|\n",
      "|       yogurt|     417.0|           5|\n",
      "|         soap|     280.0|           4|\n",
      "|floor cleaner|    2710.0|           5|\n",
      "|         soda|    1630.0|           5|\n",
      "|      Shampoo|    3148.0|           5|\n",
      "|     biscuits|     974.0|           5|\n",
      "+-------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.groupby('category_desc') \\\n",
    "             .pivot('week_id',[201718]) \\\n",
    "             .agg(F.sum('amount').alias('sum'), \\\n",
    "                  F.countDistinct('prod_id').alias('count'))\n",
    "        \n",
    "pivot_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Advanced Functionalities`\n",
    "  \n",
    "  * Sampling using limit\n",
    "  \n",
    "  \n",
    "  * Creating a random sample\n",
    "  \n",
    "  \n",
    "  * Creating a stratified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-------------+\n",
      "|prod_id|category_id|department|category_desc|\n",
      "+-------+-----------+----------+-------------+\n",
      "|     P1|         C1|        D1|         Milk|\n",
      "|     P2|         C2|        D2|      Shampoo|\n",
      "|     P3|         C3|        D1|       yogurt|\n",
      "|     P4|         C4|        D2|         soap|\n",
      "|     P5|         C5|        D2|floor cleaner|\n",
      "|     P6|         C6|        D3|     biscuits|\n",
      "|     P7|         C7|        D3|         soda|\n",
      "|     P8|         C1|        D1|         Milk|\n",
      "|     P9|         C2|        D2|      Shampoo|\n",
      "|    P10|         C3|        D1|       yogurt|\n",
      "+-------+-----------+----------+-------------+\n",
      "\n",
      "The count of rows is ->\n",
      "35\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Pulling out products data\n",
    "prods = sqlContext.table(\"an_training_easl.training1_products\")\n",
    "\n",
    "# Limiting values to 10 rows\n",
    "prods.limit(10).show()\n",
    "\n",
    "# Checking the count of rows\n",
    "print(\"The count of rows is ->\")\n",
    "print(prods.count())\n",
    "print(prods.limit(10).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of rows is ->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling a random sample\n",
    "\n",
    "# Sample function arguments are: -\n",
    "# sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "sample = prods.sample(False,0.5,222)\n",
    "\n",
    "print(\"The count of rows is ->\")\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual data distribution by dept\n",
      "\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|        D1|   10|\n",
      "|        D2|   15|\n",
      "|        D3|   10|\n",
      "+----------+-----+\n",
      "\n",
      "sampled data distribution by dept\n",
      "\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|        D1|    5|\n",
      "|        D3|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a stratified sample\n",
    "\n",
    "print \"Actual data distribution by dept\\n\"\n",
    "prods.groupBy(\"department\").count().orderBy(\"department\").show()\n",
    "\n",
    "print \"sampled data distribution by dept\\n\"\n",
    "\n",
    "# Startified sampling\n",
    "sampled = prods.sampleBy(\"department\", fractions={'D1': 0.5, 'D2': 0.2, 'D3': 0.1}, seed=0)\n",
    "\n",
    "# Aggregating counts\n",
    "sampled.groupBy(\"department\").count().orderBy(\"department\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For curious ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+-------------+---+\n",
      "|prod_id|category_id|department|category_desc| id|\n",
      "+-------+-----------+----------+-------------+---+\n",
      "|     P1|         C1|        D1|         Milk|  0|\n",
      "|     P2|         C2|        D2|      Shampoo|  1|\n",
      "|     P3|         C3|        D1|       yogurt|  2|\n",
      "|     P4|         C4|        D2|         soap|  3|\n",
      "|     P5|         C5|        D2|floor cleaner|  4|\n",
      "|     P6|         C6|        D3|     biscuits|  5|\n",
      "|     P7|         C7|        D3|         soda|  6|\n",
      "|     P8|         C1|        D1|         Milk|  7|\n",
      "|     P9|         C2|        D2|      Shampoo|  8|\n",
      "|    P10|         C3|        D1|       yogurt|  9|\n",
      "|    P11|         C4|        D2|         soap| 10|\n",
      "|    P12|         C5|        D2|floor cleaner| 11|\n",
      "|    P13|         C6|        D3|     biscuits| 12|\n",
      "|    P14|         C7|        D3|         soda| 13|\n",
      "|    P15|         C1|        D1|         Milk| 14|\n",
      "|    P16|         C2|        D2|      Shampoo| 15|\n",
      "|    P17|         C3|        D1|       yogurt| 16|\n",
      "|    P18|         C4|        D2|         soap| 17|\n",
      "|    P19|         C5|        D2|floor cleaner| 18|\n",
      "|    P20|         C6|        D3|     biscuits| 19|\n",
      "+-------+-----------+----------+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating an id column\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "df_index = prods.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n",
    "df_index.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id\n",
    "\n",
    "*A column that generates monotonically increasing 64-bit integers. The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Advanced Functionalities - Spark User Defined Functions - (UDF)`\n",
    "\n",
    "  * Use a Python function in PySpark\n",
    "  \n",
    "<span style='color:Orange'>*UDFs might sound a good option. Python UDF are not. They should be avoided if possible, because they require data serialisation and deserialization from the JVM to the python layer and are always a performance bottleneck*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to take square of a value\n",
    "def square_number(s):\n",
    "    return s * s\n",
    "\n",
    "sqlContext.udf.register(\"squaredWithPython\", square_number)\n",
    "\n",
    "# Creating a lambda function\n",
    "square_udf_int = F.udf(lambda z: square_number(z), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-------+---+------+-------+\n",
      "|transaction_id|   date_id|card_id|prod_id|qty|amount|week_id|\n",
      "+--------------+----------+-------+-------+---+------+-------+\n",
      "|      10134103|01-05-2017|    101|    P20|  1|  45.0| 201718|\n",
      "|      10162849|01-05-2017|    103|    P35|  1| 120.0| 201718|\n",
      "+--------------+----------+-------+-------+---+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing 2 rows of the transaction dataframe\n",
    "transactions = sqlContext.table(\"an_training_easl.training1_transactions\")\n",
    "transactions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|amount|int_squared|\n",
      "+------+-----------+\n",
      "|  45.0|     2025.0|\n",
      "| 120.0|    14400.0|\n",
      "+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calling the udf on the transaction dataframe\n",
    "transactions_withSquare=transactions.select('amount',square_udf_int('amount').alias('int_squared'))\n",
    "transactions_withSquare.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Advanced Functionalities - Some More String functions`\n",
    "\n",
    "  * Padding on the right of a column\n",
    "  \n",
    "  \n",
    "  * Translating values into other values\n",
    "  \n",
    "  \n",
    "  * Formatting values through concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|card_id|card_id|\n",
      "+-------+-------+\n",
      "|    101| 101000|\n",
      "|    103| 103000|\n",
      "|    105| 105000|\n",
      "|    107| 107000|\n",
      "|    111| 111000|\n",
      "|    113| 113000|\n",
      "|    115| 115000|\n",
      "|    117| 117000|\n",
      "|    120| 120000|\n",
      "|    122| 122000|\n",
      "|    123| 123000|\n",
      "|    127| 127000|\n",
      "|    133| 133000|\n",
      "|    137| 137000|\n",
      "|    191| 191000|\n",
      "|    101| 101000|\n",
      "|    103| 103000|\n",
      "|    105| 105000|\n",
      "|    107| 107000|\n",
      "|    111| 111000|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# R-padding values in card_id column\n",
    "transactions.select('card_id',rpad(transactions['card_id'],6,'0').alias('card_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|prod_id|prod_id|\n",
      "+-------+-------+\n",
      "|    P20|    AD0|\n",
      "|    P35|    AB5|\n",
      "|    P31|    ABC|\n",
      "|    P13|    ACB|\n",
      "|    P35|    AB5|\n",
      "|    P25|    AD5|\n",
      "|    P33|    ABB|\n",
      "|    P23|    ADB|\n",
      "|    P13|    ACB|\n",
      "|     P2|     AD|\n",
      "|    P24|    AD4|\n",
      "|    P12|    ACD|\n",
      "|    P35|    AB5|\n",
      "|     P9|     A9|\n",
      "|    P34|    AB4|\n",
      "|    P29|    AD9|\n",
      "|    P15|    AC5|\n",
      "|    P33|    ABB|\n",
      "|     P7|     A7|\n",
      "|    P16|    AC6|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Replacing each occurrence of a character with another character\n",
    "## In the following example every 'P' is replaced by 'A', '3' by 'B', '1' by 'C', '2' by 'D'\n",
    "transactions.select('prod_id',translate(transactions['prod_id'],'P312','ABCD').alias('prod_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|week_id|        Week|\n",
      "+-------+------------+\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "| 201718|Week: 201718|\n",
      "+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Formatting values in string\n",
    "transactions.select('week_id',F.format_string('Week: %s', transactions['week_id']).alias('Week')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Efficiency Tips`\n",
    "\n",
    "  * Broadcast Joins\n",
    "  \n",
    "  \n",
    "  * Persisting tables\n",
    "  \n",
    "  \n",
    "  * Repartitioning and Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Broadcast Joins`\n",
    "\n",
    "  * Broadcast joins are used when one of the bigger tables needs to be joined with smaller tables\n",
    "  \n",
    "  \n",
    "  * Use broadcast join of transactions table and prod table where product table is broadcasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|transaction_id|   date_id|card_id|prod_id|qty|amount|week_id|prod_id|category_id|department|category_desc|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|      10134103|01-05-2017|    101|    P20|  1|  45.0| 201718|    P20|         C6|        D3|     biscuits|\n",
      "|      10162849|01-05-2017|    103|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10187085|01-05-2017|    105|    P31|  1|  28.0| 201718|    P31|         C3|        D1|       yogurt|\n",
      "|      10413355|02-05-2017|    107|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      10543839|02-05-2017|    111|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10600256|02-05-2017|    113|    P25|  1|  40.0| 201718|    P25|         C4|        D2|         soap|\n",
      "|      10664653|02-05-2017|    115|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      11026889|02-05-2017|    117|    P23|  1| 250.0| 201718|    P23|         C2|        D2|      Shampoo|\n",
      "|      11026939|02-05-2017|    120|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      11221390|02-05-2017|    122|     P2|  1| 250.0| 201718|     P2|         C2|        D2|      Shampoo|\n",
      "|      11238203|03-05-2017|    123|    P24|  1|  35.0| 201718|    P24|         C3|        D1|       yogurt|\n",
      "|      11450230|03-05-2017|    127|    P12|  1| 110.0| 201718|    P12|         C5|        D2|floor cleaner|\n",
      "|      11531607|03-05-2017|    133|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      11620341|03-05-2017|    137|     P9|  1| 190.0| 201718|     P9|         C2|        D2|      Shampoo|\n",
      "|      11768479|03-05-2017|    191|    P34|  1|  70.0| 201718|    P34|         C6|        D3|     biscuits|\n",
      "|      12381525|04-05-2017|    101|    P29|  1|  19.0| 201718|    P29|         C1|        D1|         Milk|\n",
      "|      12389296|04-05-2017|    103|    P15|  1|  20.0| 201718|    P15|         C1|        D1|         Milk|\n",
      "|      13088125|04-05-2017|    105|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      13229377|04-05-2017|    107|     P7|  1|  35.0| 201718|     P7|         C7|        D3|         soda|\n",
      "|      13372471|04-05-2017|    111|    P16|  1| 111.0| 201718|    P16|         C2|        D2|      Shampoo|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Broadcast Join using the broadcast on the smaller dataframe\n",
    "transactions.join(F.broadcast(products), transactions.prod_id == products.prod_id, 'inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Persisting Dataframes`\n",
    "\n",
    "  * Persist dataframes if they are used multiple times and the cost of creation of these dataframes is higher\n",
    "  \n",
    "  \n",
    "  * Eg. Persist prod_trans table to avoid the table to be recreated when it's used after the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|transaction_id|   date_id|card_id|prod_id|qty|amount|week_id|prod_id|category_id|department|category_desc|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|      10134103|01-05-2017|    101|    P20|  1|  45.0| 201718|    P20|         C6|        D3|     biscuits|\n",
      "|      10162849|01-05-2017|    103|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10187085|01-05-2017|    105|    P31|  1|  28.0| 201718|    P31|         C3|        D1|       yogurt|\n",
      "|      10413355|02-05-2017|    107|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      10543839|02-05-2017|    111|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10600256|02-05-2017|    113|    P25|  1|  40.0| 201718|    P25|         C4|        D2|         soap|\n",
      "|      10664653|02-05-2017|    115|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      11026889|02-05-2017|    117|    P23|  1| 250.0| 201718|    P23|         C2|        D2|      Shampoo|\n",
      "|      11026939|02-05-2017|    120|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      11221390|02-05-2017|    122|     P2|  1| 250.0| 201718|     P2|         C2|        D2|      Shampoo|\n",
      "|      11238203|03-05-2017|    123|    P24|  1|  35.0| 201718|    P24|         C3|        D1|       yogurt|\n",
      "|      11450230|03-05-2017|    127|    P12|  1| 110.0| 201718|    P12|         C5|        D2|floor cleaner|\n",
      "|      11531607|03-05-2017|    133|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      11620341|03-05-2017|    137|     P9|  1| 190.0| 201718|     P9|         C2|        D2|      Shampoo|\n",
      "|      11768479|03-05-2017|    191|    P34|  1|  70.0| 201718|    P34|         C6|        D3|     biscuits|\n",
      "|      12381525|04-05-2017|    101|    P29|  1|  19.0| 201718|    P29|         C1|        D1|         Milk|\n",
      "|      12389296|04-05-2017|    103|    P15|  1|  20.0| 201718|    P15|         C1|        D1|         Milk|\n",
      "|      13088125|04-05-2017|    105|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      13229377|04-05-2017|    107|     P7|  1|  35.0| 201718|     P7|         C7|        D3|         soda|\n",
      "|      13372471|04-05-2017|    111|    P16|  1| 111.0| 201718|    P16|         C2|        D2|      Shampoo|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Persisting dataframe and calling an action on it\n",
    "prod_trans = prod_trans.persist()\n",
    "\n",
    "# An action for the first time takes some time\n",
    "prod_trans.show()\n",
    "#unpersist it using DataFrame.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transaction_id: bigint, date_id: string, card_id: bigint, prod_id: string, qty: bigint, amount: float, week_id: int, prod_id: string, category_id: string, department: string, category_desc: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Practice - Unpersist the dataset when it is no longer used (multiple times)\n",
    "prod_trans.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Repartition and Coalesce`\n",
    "\n",
    "  * Repartition the table using repartition(no_of_partitions) if you want equal distibutions across cluster\n",
    "  \n",
    "  \n",
    "  * Coalesce the table if you dont want shuffle overhead unlike repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(products.rdd.getNumPartitions())\n",
    "print(products.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-partition the dataframe into 2\n",
    "products_rp = products.repartition(2)\n",
    "products_rp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coalesce the dataframe into 1\n",
    "products_cl = products.coalesce(1)\n",
    "products_cl.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark APIs vs. SQL\n",
    "\n",
    "#### Efficiency Tip - Spark APIs should be preferred - it gives you much more control on what happens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important - Spark IS NOT a RDBMS, so avoid writing long SQL statements. Complicated SQL code is often a good candidate for performance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------+---+--------------+-------------+\n",
      "|week_id|prod_id|card_id|amount|qty|transaction_id|category_desc|\n",
      "+-------+-------+-------+------+---+--------------+-------------+\n",
      "| 201719|     P9|    113| 190.0|  1|      28007758|      Shampoo|\n",
      "| 201719|     P5|    115| 150.0|  1|      28014164|floor cleaner|\n",
      "| 201719|    P18|    117|  25.0|  1|      28221466|         soap|\n",
      "| 201719|    P28|    127|  35.0|  1|      26959555|         soda|\n",
      "| 201719|     P1|    133|  21.0|  1|      25884734|         Milk|\n",
      "| 201719|    P17|    133|  55.0|  1|      27127095|       yogurt|\n",
      "| 201719|    P12|    137| 110.0|  1|      26031912|floor cleaner|\n",
      "| 201719|     P6|    137|  27.0|  1|      27147180|     biscuits|\n",
      "| 201719|     P1|    191|  21.0|  1|      26136122|         Milk|\n",
      "| 201719|    P32|    191|  30.0|  1|      27160634|         soap|\n",
      "| 201719|     P4|    203|  40.0|  1|      26275286|         soap|\n",
      "| 201719|     P7|    203|  35.0|  1|      27360181|         soda|\n",
      "| 201719|    P10|    203|  28.0|  1|      28353601|       yogurt|\n",
      "| 201719|    P33|    221| 110.0|  1|      26511408|floor cleaner|\n",
      "| 201719|    P24|    221|  35.0|  1|      27681840|       yogurt|\n",
      "| 201719|    P16|    221| 111.0|  1|      28545795|      Shampoo|\n",
      "| 201719|     P1|    246|  21.0|  1|      26652722|         Milk|\n",
      "| 201719|     P8|    246|  19.0|  1|      28007199|         Milk|\n",
      "| 201719|     P8|    273|  19.0|  1|      26638041|         Milk|\n",
      "| 201719|     P6|    273|  27.0|  1|      27920644|     biscuits|\n",
      "+-------+-------+-------+------+---+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL to create a dataframe\n",
    "df = sqlContext.sql(\"\"\"select a.week_id,a.prod_id,a.card_id,a.amount,qty,a.transaction_id, b.category_desc\n",
    "                       from an_training_easl.training1_transactions a\n",
    "                       inner join \n",
    "                       (select distinct prod_id, category_desc from an_training_easl.training1_products) b\n",
    "                       on a.prod_id = b.prod_id\n",
    "                       where week_id > 201718 and a.card_id is not null\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the type of object created\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnDF = transactions.where(\"date_id between '01-05-2017' and '03-05-2017'\") \\\n",
    "                    .join(F.broadcast(products), transactions.prod_id == products.prod_id, 'inner') \\\n",
    "                    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|transaction_id|   date_id|card_id|prod_id|qty|amount|week_id|prod_id|category_id|department|category_desc|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "|      10134103|01-05-2017|    101|    P20|  1|  45.0| 201718|    P20|         C6|        D3|     biscuits|\n",
      "|      10162849|01-05-2017|    103|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10187085|01-05-2017|    105|    P31|  1|  28.0| 201718|    P31|         C3|        D1|       yogurt|\n",
      "|      10413355|02-05-2017|    107|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      10543839|02-05-2017|    111|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      10600256|02-05-2017|    113|    P25|  1|  40.0| 201718|    P25|         C4|        D2|         soap|\n",
      "|      10664653|02-05-2017|    115|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|\n",
      "|      11026889|02-05-2017|    117|    P23|  1| 250.0| 201718|    P23|         C2|        D2|      Shampoo|\n",
      "|      11026939|02-05-2017|    120|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|\n",
      "|      11221390|02-05-2017|    122|     P2|  1| 250.0| 201718|     P2|         C2|        D2|      Shampoo|\n",
      "|      11238203|03-05-2017|    123|    P24|  1|  35.0| 201718|    P24|         C3|        D1|       yogurt|\n",
      "|      11450230|03-05-2017|    127|    P12|  1| 110.0| 201718|    P12|         C5|        D2|floor cleaner|\n",
      "|      11531607|03-05-2017|    133|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|\n",
      "|      11620341|03-05-2017|    137|     P9|  1| 190.0| 201718|     P9|         C2|        D2|      Shampoo|\n",
      "|      11768479|03-05-2017|    191|    P34|  1|  70.0| 201718|    P34|         C6|        D3|     biscuits|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trnDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(trnDF['date_id']).orderBy(trnDF['amount'].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn2 = trnDF.select('*', F.rank().over(w).alias(\"day_rank\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+--------+\n",
      "|transaction_id|   date_id|card_id|prod_id|qty|amount|week_id|prod_id|category_id|department|category_desc|day_rank|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+--------+\n",
      "|      11026889|02-05-2017|    117|    P23|  1| 250.0| 201718|    P23|         C2|        D2|      Shampoo|       1|\n",
      "|      11221390|02-05-2017|    122|     P2|  1| 250.0| 201718|     P2|         C2|        D2|      Shampoo|       1|\n",
      "|      10543839|02-05-2017|    111|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|       3|\n",
      "|      10664653|02-05-2017|    115|    P33|  1| 110.0| 201718|    P33|         C5|        D2|floor cleaner|       4|\n",
      "|      10413355|02-05-2017|    107|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|       5|\n",
      "|      11026939|02-05-2017|    120|    P13|  1|  70.0| 201718|    P13|         C6|        D3|     biscuits|       5|\n",
      "|      10600256|02-05-2017|    113|    P25|  1|  40.0| 201718|    P25|         C4|        D2|         soap|       7|\n",
      "|      10162849|01-05-2017|    103|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|       1|\n",
      "|      10134103|01-05-2017|    101|    P20|  1|  45.0| 201718|    P20|         C6|        D3|     biscuits|       2|\n",
      "|      10187085|01-05-2017|    105|    P31|  1|  28.0| 201718|    P31|         C3|        D1|       yogurt|       3|\n",
      "|      11620341|03-05-2017|    137|     P9|  1| 190.0| 201718|     P9|         C2|        D2|      Shampoo|       1|\n",
      "|      11531607|03-05-2017|    133|    P35|  1| 120.0| 201718|    P35|         C7|        D3|         soda|       2|\n",
      "|      11450230|03-05-2017|    127|    P12|  1| 110.0| 201718|    P12|         C5|        D2|floor cleaner|       3|\n",
      "|      11768479|03-05-2017|    191|    P34|  1|  70.0| 201718|    P34|         C6|        D3|     biscuits|       4|\n",
      "|      11238203|03-05-2017|    123|    P24|  1|  35.0| 201718|    P24|         C3|        D1|       yogurt|       5|\n",
      "+--------------+----------+-------+-------+---+------+-------+-------+-----------+----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trn2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.2.0",
   "language": "python",
   "name": "pyspark-2.2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
